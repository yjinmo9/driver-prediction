#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Holdout ì˜ˆì¸¡ê°’ ë¶„í¬ ë¶„ì„ ë° íŒŒë¼ë¯¸í„° ì¡°ì • ì‹œë®¬ë ˆì´ì…˜"""

import os
import warnings
import numpy as np
import pandas as pd
from sklearn.metrics import roc_auc_score, brier_score_loss
from sklearn.model_selection import train_test_split

from src.config import (
    DATA_DIR,
    MODEL_DIR,
    RANDOM_SEED,
    VALID_SIZE,
)
from src.data_utils import read_index_files, read_feature_files
from src.feature_engineer import (
    get_drop_columns,
    split_numeric_categorical,
    build_preprocessor,
    add_rowwise_features,
    preprocess_A_v2,
    preprocess_B_v2,
)
from src.model_utils import (
    build_and_train_ensemble,
    TemperatureScaler,
    CalibratedWithTemperature,
)
from src.evaluate import compute_ece, compute_final_score
import joblib

warnings.filterwarnings("ignore")


def analyze_predictions(y_true, y_pred, name="Model"):
    """ì˜ˆì¸¡ê°’ ë¶„í¬ ë¶„ì„"""
    print(f"\n{'='*60}")
    print(f"[{name}] ì˜ˆì¸¡ê°’ ë¶„í¬ ë¶„ì„")
    print(f"{'='*60}")
    
    # ê¸°ë³¸ í†µê³„
    print(f"\nğŸ“Š ê¸°ë³¸ í†µê³„:")
    print(f"  ì˜ˆì¸¡ê°’ í‰ê· : {np.mean(y_pred):.6f}")
    print(f"  ì˜ˆì¸¡ê°’ ì¤‘ì•™ê°’: {np.median(y_pred):.6f}")
    print(f"  ì˜ˆì¸¡ê°’ í‘œì¤€í¸ì°¨: {np.std(y_pred):.6f}")
    print(f"  ì˜ˆì¸¡ê°’ ìµœì†Œ: {np.min(y_pred):.6f}")
    print(f"  ì˜ˆì¸¡ê°’ ìµœëŒ€: {np.max(y_pred):.6f}")
    print(f"  ì˜ˆì¸¡ê°’ ë²”ìœ„: {np.max(y_pred) - np.min(y_pred):.6f}")
    
    # ë¶„ìœ„ìˆ˜
    percentiles = [10, 25, 50, 75, 90, 95, 99]
    print(f"\nğŸ“ˆ ë¶„ìœ„ìˆ˜:")
    for p in percentiles:
        val = np.percentile(y_pred, p)
        print(f"  {p}%: {val:.6f}")
    
    # ì‹¤ì œ Label ë¶„í¬
    print(f"\nğŸ¯ ì‹¤ì œ Label ë¶„í¬:")
    print(f"  Label í‰ê· : {np.mean(y_true):.6f}")
    print(f"  Label ë¹„ìœ¨ (0/1): {(y_true==0).sum()}/{((y_true==1).sum())} ({np.mean(y_true==0):.2%}/{np.mean(y_true==1):.2%})")
    
    # ì˜ˆì¸¡ê°’ vs ì‹¤ì œ Label
    print(f"\nğŸ“‰ ì˜ˆì¸¡ê°’ vs ì‹¤ì œ Label:")
    print(f"  ì˜ˆì¸¡ê°’ í‰ê·  - Label í‰ê· : {np.mean(y_pred) - np.mean(y_true):.6f}")
    
    # í‰ê°€ ì§€í‘œ
    auc = roc_auc_score(y_true, y_pred)
    brier = brier_score_loss(y_true, y_pred)
    ece = compute_ece(y_true, y_pred, n_bins=15)
    final = compute_final_score(auc, brier, ece)
    
    print(f"\nğŸ“Š í‰ê°€ ì§€í‘œ:")
    print(f"  AUC: {auc:.5f}")
    print(f"  Brier: {brier:.5f}")
    print(f"  ECE: {ece:.5f}")
    print(f"  Final: {final:.5f}")
    
    return {
        "mean": np.mean(y_pred),
        "median": np.median(y_pred),
        "std": np.std(y_pred),
        "min": np.min(y_pred),
        "max": np.max(y_pred),
        "range": np.max(y_pred) - np.min(y_pred),
        "auc": auc,
        "brier": brier,
        "ece": ece,
        "final": final,
    }


def simulate_parameter_adjustment(
    X_tr, y_tr, X_val, y_val, 
    base_params, adjustments, which="A"
):
    """íŒŒë¼ë¯¸í„° ì¡°ì • ì‹œë®¬ë ˆì´ì…˜"""
    print(f"\n{'='*60}")
    print(f"[{which}] íŒŒë¼ë¯¸í„° ì¡°ì • ì‹œë®¬ë ˆì´ì…˜")
    print(f"{'='*60}")
    
    results = []
    
    for adj_name, adj_params in adjustments.items():
        print(f"\nğŸ”§ ì¡°ì •: {adj_name}")
        print(f"   íŒŒë¼ë¯¸í„°: {adj_params}")
        
        # íŒŒë¼ë¯¸í„° ë³‘í•©
        params = base_params.copy()
        params.update(adj_params)
        
        # ëª¨ë¸ í•™ìŠµ
        try:
            ensemble = build_and_train_ensemble(X_tr, y_tr, custom_params=params)
            
            # ì˜ˆì¸¡
            val_proba = np.clip(ensemble.predict_proba(X_val)[:, 1], 1e-7, 1-1e-7)
            
            # ì˜¨ë„ ìŠ¤ì¼€ì¼ë§
            temp = TemperatureScaler()
            temp.fit(y_val, val_proba)
            ensemble = CalibratedWithTemperature(ensemble, temp)
            val_proba = np.clip(ensemble.predict_proba(X_val)[:, 1], 1e-7, 1-1e-7)
            
            # í‰ê°€
            stats = analyze_predictions(y_val, val_proba, f"{which}_{adj_name}")
            stats["adj_name"] = adj_name
            stats["params"] = params.copy()
            results.append(stats)
            
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")
            continue
    
    return results


def main():
    print("="*60)
    print("Holdout ì˜ˆì¸¡ê°’ ë¶„í¬ ë¶„ì„ ë° íŒŒë¼ë¯¸í„° ì¡°ì • ì‹œë®¬ë ˆì´ì…˜")
    print("="*60)
    
    # ë°ì´í„° ë¡œë“œ
    train_idx, _ = read_index_files()
    A_train_feat, B_train_feat = read_feature_files("train")
    
    # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
    A_train_feat = preprocess_A_v2(A_train_feat)
    B_train_feat = preprocess_B_v2(B_train_feat)
    
    # A ëª¨ë¸ ë¶„ì„
    print("\n" + "="*60)
    print("A ëª¨ë¸ ë¶„ì„")
    print("="*60)
    
    A_idx = train_idx[train_idx["Test"] == "A"].copy()
    A_df = A_idx.merge(A_train_feat, on="Test_id", how="left", validate="1:1")
    
    if 'Test_x' in A_df.columns or 'Test_y' in A_df.columns:
        if 'Test_x' in A_df.columns:
            A_df.rename(columns={'Test_x': 'Test'}, inplace=True)
        A_df.drop(columns=[c for c in ['Test_y'] if c in A_df.columns], inplace=True, errors='ignore')
    A_df = A_df.loc[:, ~A_df.columns.duplicated()]
    
    drop_cols = ["Test_id", "Label"]
    drop_cols += [c for c in ['Test_x', 'Test_y'] if c in A_df.columns]
    drop_cols = [c for c in drop_cols if c in A_df.columns]
    
    feature_cols = [c for c in A_df.columns if c not in drop_cols]
    A_df = add_rowwise_features(A_df, feature_cols)
    
    X_A = A_df.drop(columns=drop_cols)
    y_A = A_df["Label"].astype(int).values
    
    X_A_tr, X_A_val, y_A_tr, y_A_val = train_test_split(
        X_A, y_A, test_size=VALID_SIZE, random_state=RANDOM_SEED, stratify=y_A
    )
    
    # ì „ì²˜ë¦¬
    preproc_A = build_preprocessor(A_df, feature_cols)
    X_A_tr_t = preproc_A.fit_transform(X_A_tr)
    X_A_val_t = preproc_A.transform(X_A_val)
    
    # í˜„ì¬ ëª¨ë¸ (ë”_ê³µê²©ì _3 íŒŒë¼ë¯¸í„° - ìµœê·¼ ì ìš©í•œ ê²ƒ)
    params_current = {
        "learning_rate": 0.05,
        "max_iter": 1200,
        "max_depth": None,
        "max_leaf_nodes": 31,
        "min_samples_leaf": 30,
        "l2_regularization": 0.6,
        "early_stopping": True,
        "validation_fraction": 0.15,
        "n_iter_no_change": 45,
        "class_weight": None,
    }
    
    print("\n[í˜„ì¬ ëª¨ë¸] ë”_ê³µê²©ì _3 íŒŒë¼ë¯¸í„°")
    ensemble_A = build_and_train_ensemble(X_A_tr_t, y_A_tr, custom_params=params_current)
    val_proba_A = np.clip(ensemble_A.predict_proba(X_A_val_t)[:, 1], 1e-7, 1-1e-7)
    temp_A = TemperatureScaler()
    temp_A.fit(y_A_val, val_proba_A)
    ensemble_A = CalibratedWithTemperature(ensemble_A, temp_A)
    val_proba_A = np.clip(ensemble_A.predict_proba(X_A_val_t)[:, 1], 1e-7, 1-1e-7)
    
    stats_A = analyze_predictions(y_A_val, val_proba_A, "A_í˜„ì¬")
    
    # íŒŒë¼ë¯¸í„° ì¡°ì • ì‹œë®¬ë ˆì´ì…˜
    adjustments = {
        "ë”_ê³µê²©ì _1": {
            "learning_rate": 0.05,
            "class_weight": None,  # balanced ì œê±°
        },
        "ë”_ê³µê²©ì _2": {
            "learning_rate": 0.06,
            "max_iter": 1200,
            "class_weight": None,
        },
        "ë”_ê³µê²©ì _3": {
            "learning_rate": 0.05,
            "max_iter": 1200,
            "min_samples_leaf": 30,  # ë” ì‘ê²Œ
            "class_weight": None,
        },
        "ë§¤ìš°_ê³µê²©ì _1": {
            "learning_rate": 0.07,
            "max_iter": 1500,
            "min_samples_leaf": 20,
            "l2_regularization": 0.4,
            "class_weight": None,
        },
        "ë§¤ìš°_ê³µê²©ì _2": {
            "learning_rate": 0.08,
            "max_iter": 1500,
            "min_samples_leaf": 15,
            "l2_regularization": 0.3,
            "max_leaf_nodes": 63,
            "class_weight": None,
        },
        "ë§¤ìš°_ê³µê²©ì _3": {
            "learning_rate": 0.06,
            "max_iter": 1800,
            "min_samples_leaf": 10,
            "l2_regularization": 0.2,
            "max_leaf_nodes": 127,
            "class_weight": None,
        },
    }
    
    results_A = simulate_parameter_adjustment(
        X_A_tr_t, y_A_tr, X_A_val_t, y_A_val,
        params_current, adjustments, "A"
    )
    
    # B ëª¨ë¸ ë¶„ì„
    print("\n" + "="*60)
    print("B ëª¨ë¸ ë¶„ì„")
    print("="*60)
    
    B_idx = train_idx[train_idx["Test"] == "B"].copy()
    B_df = B_idx.merge(B_train_feat, on="Test_id", how="left", validate="1:1")
    
    if 'Test_x' in B_df.columns or 'Test_y' in B_df.columns:
        if 'Test_x' in B_df.columns:
            B_df.rename(columns={'Test_x': 'Test'}, inplace=True)
        B_df.drop(columns=[c for c in ['Test_y'] if c in B_df.columns], inplace=True, errors='ignore')
    B_df = B_df.loc[:, ~B_df.columns.duplicated()]
    
    drop_cols = ["Test_id", "Label"]
    drop_cols += [c for c in ['Test_x', 'Test_y'] if c in B_df.columns]
    drop_cols = [c for c in drop_cols if c in B_df.columns]
    
    feature_cols = [c for c in B_df.columns if c not in drop_cols]
    B_df = add_rowwise_features(B_df, feature_cols)
    
    X_B = B_df.drop(columns=drop_cols)
    y_B = B_df["Label"].astype(int).values
    
    X_B_tr, X_B_val, y_B_tr, y_B_val = train_test_split(
        X_B, y_B, test_size=VALID_SIZE, random_state=RANDOM_SEED, stratify=y_B
    )
    
    # ì „ì²˜ë¦¬
    preproc_B = build_preprocessor(B_df, feature_cols)
    X_B_tr_t = preproc_B.fit_transform(X_B_tr)
    X_B_val_t = preproc_B.transform(X_B_val)
    
    # í˜„ì¬ ëª¨ë¸ (ë”_ê³µê²©ì _3 íŒŒë¼ë¯¸í„° - ìµœê·¼ ì ìš©í•œ ê²ƒ)
    params_B_current = {
        "learning_rate": 0.05,
        "max_iter": 1200,
        "max_depth": None,
        "max_leaf_nodes": 31,
        "min_samples_leaf": 30,
        "l2_regularization": 0.7,
        "early_stopping": True,
        "validation_fraction": 0.15,
        "n_iter_no_change": 50,
        "class_weight": None,
    }
    
    print("\n[í˜„ì¬ ëª¨ë¸] ë”_ê³µê²©ì _3 íŒŒë¼ë¯¸í„°")
    ensemble_B = build_and_train_ensemble(X_B_tr_t, y_B_tr, custom_params=params_B_current)
    val_proba_B = np.clip(ensemble_B.predict_proba(X_B_val_t)[:, 1], 1e-7, 1-1e-7)
    temp_B = TemperatureScaler()
    temp_B.fit(y_B_val, val_proba_B)
    ensemble_B = CalibratedWithTemperature(ensemble_B, temp_B)
    val_proba_B = np.clip(ensemble_B.predict_proba(X_B_val_t)[:, 1], 1e-7, 1-1e-7)
    
    stats_B = analyze_predictions(y_B_val, val_proba_B, "B_í˜„ì¬")
    
    # íŒŒë¼ë¯¸í„° ì¡°ì • ì‹œë®¬ë ˆì´ì…˜
    results_B = simulate_parameter_adjustment(
        X_B_tr_t, y_B_tr, X_B_val_t, y_B_val,
        params_B_current, adjustments, "B"
    )
    
    # ì¢…í•© ë¦¬í¬íŠ¸
    print("\n" + "="*60)
    print("ì¢…í•© ë¦¬í¬íŠ¸")
    print("="*60)
    
    print("\nğŸ“Š A ëª¨ë¸ ë¹„êµ:")
    print(f"  í˜„ì¬ Final: {stats_A['final']:.5f}")
    for r in results_A:
        print(f"  {r['adj_name']}: {r['final']:.5f} (ì°¨ì´: {r['final'] - stats_A['final']:+.5f})")
    
    print("\nğŸ“Š B ëª¨ë¸ ë¹„êµ:")
    print(f"  í˜„ì¬ Final: {stats_B['final']:.5f}")
    for r in results_B:
        print(f"  {r['adj_name']}: {r['final']:.5f} (ì°¨ì´: {r['final'] - stats_B['final']:+.5f})")
    
    print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    print("  - ì˜ˆì¸¡ê°’ ë²”ìœ„ê°€ ë„“ì–´ì§€ë©´ AUCê°€ ê°œì„ ë  ìˆ˜ ìˆìŒ")
    print("  - class_weight=Noneìœ¼ë¡œ ë³€ê²½í•˜ë©´ ë” ê³µê²©ì ìœ¼ë¡œ ì˜ˆì¸¡")
    print("  - learning_rate ì¦ê°€ë¡œ ë” ë¹ ë¥´ê²Œ í•™ìŠµ")
    print("  - min_samples_leaf ê°ì†Œë¡œ ë” ì„¸ë°€í•œ ë¶„í• ")


if __name__ == "__main__":
    main()

